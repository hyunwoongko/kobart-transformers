## KoBART-Transformers
- SKTì—ì„œ ê³µê°œí•œ KoBARTë¥¼ í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ transformersë¡œ í¬íŒ…í•˜ì˜€ìŠµë‹ˆë‹¤.
<br><br>

### Install (Optional)
- `BartModel`ê³¼ `PreTrainedTokenizerFast`ë¥¼ ì´ìš©í•˜ë©´ ì„¤ì¹˜í•˜ì‹¤ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
```console
pip install kobart-transformers
```
<br>

### Tokenizer
- `PreTrainedTokenizerFast`ë¥¼ ì´ìš©í•˜ì—¬ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
- `PreTrainedTokenizerFast.from_pretrained("hyunwoongko/kobart")`ì™€ ë™ì¼í•©ë‹ˆë‹¤.
```python
>>> from kobart_transformers import get_kobart_tokenizer
>>> # from transformers import PreTrainedTokenizerFast

>>> kobart_tokenizer = get_kobart_tokenizer()
>>> # kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained("hyunwoongko/kobart")

>>> kobart_tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ BART ì…ë‹ˆë‹¤.ğŸ¤£:)l^o")
['â–ì•ˆë…•í•˜', 'ì„¸ìš”.', 'â–í•œêµ­ì–´', 'â–B', 'A', 'R', 'T', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ¤£', ':)', 'l^o']
```
<br>

### Model
- `BartModel`ì„ ì´ìš©í•˜ì—¬ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
- `BartModel.from_pretrained("hyunwoongko/kobart")`ì™€ ë™ì¼í•©ë‹ˆë‹¤.
```python
>>> from kobart_transformers import get_kobart_model, get_kobart_tokenizer
>>> # from transformers import BartModel

>>> kobart_tokenizer = get_kobart_tokenizer()
>>> model = get_kobart_model()
>>> # model = BartModel.from_pretrained("hyunwoongko/kobart")

>>> inputs = kobart_tokenizer(['ì•ˆë…•í•˜ì„¸ìš”.'], return_tensors='pt')
>>> model(inputs['input_ids'])
Seq2SeqModelOutput(last_hidden_state=tensor([[[-0.4488, -4.3651,  3.2349,  ...,  5.8916,  4.0497,  3.5468],
         [-0.4096, -4.6106,  2.7189,  ...,  6.1745,  2.9832,  3.0930]]],
       grad_fn=<TransposeBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.4624, -0.2475,  0.0902,  ...,  0.1127,  0.6529,  0.2203],
         [ 0.4538, -0.2948,  0.2556,  ..., -0.0442,  0.6858,  0.4372]]],
       grad_fn=<TransposeBackward0>), encoder_hidden_states=None, encoder_attentions=None)
```
<br>

### For Seq2Seq Training
- seq2seq í•™ìŠµì‹œì—ëŠ” ì•„ë˜ì™€ ê°™ì´ `get_kobart_for_conditional_generation()`ì„ ì´ìš©í•©ë‹ˆë‹¤.
- `BartForConditionalGeneration.from_pretrained("hyunwoongko/kobart")`ì™€ ë™ì¼í•©ë‹ˆë‹¤.
```python
>>> from kobart_transformers import get_kobart_for_conditional_generation
>>> # from transformers import BartForConditionalGeneration

>>> model = get_kobart_for_conditional_generation()
>>> # model = BartForConditionalGeneration.from_pretrained("hyunwoongko/kobart")
```
<br>

### Updates Notes
#### version 0.1
- `pad` í† í°ì´ ì„¤ì •ë˜ì§€ ì•Šì€ ì—ëŸ¬ë¥¼ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤.
```python
from kobart import get_kobart_tokenizer
kobart_tokenizer = get_kobart_tokenizer()
kobart_tokenizer(["í•œêµ­ì–´", "BART ëª¨ë¸ì„", "ì†Œê°œí•©ë‹ˆë‹¤."], truncation=True, padding=True)
{
'input_ids': [[28324, 3, 3, 3, 3], [15085, 264, 281, 283, 24224], [15630, 20357, 3, 3, 3]], 
'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 
'attention_mask': [[1, 0, 0, 0, 0], [1, 1, 1, 1, 1], [1, 1, 0, 0, 0]]
}
```
#### version 0.1.3
- `get_kobart_for_conditional_generation()`ë¥¼ `__init__.py`ì— ë“±ë¡í•˜ì˜€ìŠµë‹ˆë‹¤.

#### version 0.1.4
- ëˆ„ë½ë˜ì—ˆë˜ `special_tokens_map.json`ì„ ì¶”ê°€í•˜ì˜€ìŠµë‹ˆë‹¤.
- ì´ì œ `pip install` ì—†ì´ KoBARTë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
<br>

### Reference
- [SKT KoBART](https://github.com/SKT-AI/KoBART)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
