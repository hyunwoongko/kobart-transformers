## KoBart-Transformers
- SKTì—ì„œ ê³µê°œí•œ KoBartë¥¼ í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ transformersë¡œ í¬íŒ…í•˜ì˜€ìŠµë‹ˆë‹¤.
<br><br>

### Install (Optional)
- ì„¤ì¹˜ ì—†ì´ `Model/Tokenizer.from_pretrained("hyunwoongko/kobart")`ë¥¼ ì‚¬ìš©í•´ë„ ë™ì¼í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.
- +) `hyunwoongko/kobart`ê°€ ê¸°ì–µë‚˜ì§€ ì•Šìœ¼ì‹¤ ëˆ„êµ°ê°€ë¥¼ ìœ„í•´ ì œê³µí•˜ëŠ” íŒ¨í‚¤ì§€ì´ë‹ˆ ì„¤ì¹˜ëŠ” ì„ íƒì‚¬í•­ì…ë‹ˆë‹¤.
```consol
pip install kobart-transformers
```
<br>

### Tokenizer
- `PreTrainedTokenizerFast`ë¥¼ ì´ìš©í•˜ì—¬ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
- `PreTrainedTokenizerFast.from_pretrained("hyunwoongko/kobart")`ì™€ ë™ì¼í•©ë‹ˆë‹¤.
```python
>>> from kobart_transformers import get_kobart_tokenizer
>>> # from transformers import PreTrainedTokenizerFast

>>> kobart_tokenizer = get_kobart_tokenizer()
>>> # kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained("hyunwoongko/kobart")

>>> kobart_tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ BART ì…ë‹ˆë‹¤.ğŸ¤£:)l^o")
['â–ì•ˆë…•í•˜', 'ì„¸ìš”.', 'â–í•œêµ­ì–´', 'â–B', 'A', 'R', 'T', 'â–ì…', 'ë‹ˆë‹¤.', 'ğŸ¤£', ':)', 'l^o']
```
<br>

### Model
- `BartModel`ì„ ì´ìš©í•˜ì—¬ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
- `BartModel.from_pretrained("hyunwoongko/kobart")ì™€ ë™ì¼í•©ë‹ˆë‹¤.`
```python
>>> from kobart_transformers import get_kobart_model, get_kobart_tokenizer
>>> # from transformers import BartModel, PretrainedTokenizerFast

>>> kobart_tokenizer = get_kobart_tokenizer()
>>> # kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained("hyunwoongko/kobart")

>>> model = get_kobart_model()
>>> # model = BartModel.from_pretrained("hyunwoongko/kobart")

>>> inputs = kobart_tokenizer(['ì•ˆë…•í•˜ì„¸ìš”.'], return_tensors='pt')
>>> model(inputs['input_ids'])
Seq2SeqModelOutput(last_hidden_state=tensor([[[-0.4488, -4.3651,  3.2349,  ...,  5.8916,  4.0497,  3.5468],
         [-0.4096, -4.6106,  2.7189,  ...,  6.1745,  2.9832,  3.0930]]],
       grad_fn=<TransposeBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.4624, -0.2475,  0.0902,  ...,  0.1127,  0.6529,  0.2203],
         [ 0.4538, -0.2948,  0.2556,  ..., -0.0442,  0.6858,  0.4372]]],
       grad_fn=<TransposeBackward0>), encoder_hidden_states=None, encoder_attentions=None)
```
<br>

### Reference
- [SKT KoBart](https://github.com/SKT-AI/KoBART)
- [Huggingface Transformers](https://github.com/huggingface/transformers)
